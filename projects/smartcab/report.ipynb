{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmartCab Report\n",
    "\n",
    "## Question 1\n",
    "> Observe what you see with the agent's behavior as it takes random actions. Does the smartcab eventually make it to the destination? Are there any other interesting observations to note?\n",
    "\n",
    "I haven't seen the smartcab meet the destination taking random actions before the time limit expiring.\n",
    "\n",
    "The smartcab seems to spent about half its time stopped at intersections rather than moving.\n",
    "\n",
    "Probability of moving:\n",
    "Red light -> None, Forward (2/5)\n",
    "Green light -> None (1/5)\n",
    "3/10 possibilities at a random light will keep the car stopped. That's definitely smaller than what I observe. There must be something about the orientation of the lights that means the probability of hitting a red light vs. green light is not equal (when you see a red, you see it for many turns, while you pass through a green quickly). That means the algorithm will maybe make right-turns at red lights to get out of them.\n",
    "\n",
    "## Question 2\n",
    "\n",
    "> QUESTION: What states have you identified that are appropriate for modeling the smartcab and environment? Why do you believe each of these states to be appropriate for this problem?\n",
    "\n",
    "This was my code for choosing the state. I combined inputs to make one of 64 possible states:\n",
    "\n",
    "```\n",
    "oncoming_blocking_left = inputs['oncoming'] in ['right', 'forward']\n",
    "if oncoming_blocking_left:\n",
    "    oncoming = \"oncoming blocking left\"\n",
    "elif inputs['oncoming']:\n",
    "    oncoming = \"True\"\n",
    "else:\n",
    "    oncoming = \"False\"\n",
    "\n",
    "left = str(inputs['left'] is None)\n",
    "right = str(inputs['right'] is None)\n",
    "\n",
    "# 3 x 3 x 2 x 2 x 2 possibilities = 64 possible states\n",
    "states = [self.next_waypoint, oncoming, left, right, inputs['light']]\n",
    "\n",
    "self.state = ','.join(states)\n",
    "```\n",
    "\n",
    "I wanted to leave room for the learner to learn the rules, while giving so many states that it would never have a chance to learn them all. I reduced the possible state combinations coming from left, right and oncoming to what is relevant.\n",
    "\n",
    "I would have enjoyed trying the states \"follow the rules\" vs. \"don't follow the rules\" x (time_to_destination - time_remaining) because it would be less obvious what the optimal strategy is. Unfortunately, time_to_destination is not part of the input set.\n",
    "\n",
    "## Question 3\n",
    "\n",
    "> OPTIONAL: How many states in total exist for the smartcab in this environment? Does this number seem reasonable given that the goal of Q-Learning is to learn and make informed decisions about each state? Why or why not?\n",
    "\n",
    "If we don't know how the other agents work or they are random, the number of states is exponentially related to the deadline for the smartcab in this environment. Imagine an input was: `{'light': 'red', 'left': 'forward', ...}`. The next turns state would be \"Last turn forward car on left and...\", the turn after would be \"Two turns ago forward car on left and...\". This would have to be combined with every encounter with a car.\n",
    "\n",
    "This is obviously not reasonable for Q-learning, because the difference between most of the states will be completely negligible and we would at best ignore them, and at worst overfit on them.\n",
    "\n",
    "## Question 4\n",
    "\n",
    "> QUESTION: What changes do you notice in the agent's behavior when compared to the basic driving agent when random actions were always taken? Why is this behavior occurring?\n",
    "\n",
    "The agent quickly learns to go through green lights and not to go through red lights. There is a big incentive / disincentive to perform those actions and they happen very regularly, so the learner has an opportunity to figure those strategies out quickly.\n",
    "\n",
    "## Question 5\n",
    "\n",
    "> QUESTION: Report the different values for the parameters tuned in your basic implementation of Q-Learning. For which set of parameters does the agent perform best? How well does the final driving agent perform?\n",
    "\n",
    "\n",
    "## Question 6\n",
    "\n",
    "> QUESTION: Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties? How would you describe an optimal policy for this problem?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}